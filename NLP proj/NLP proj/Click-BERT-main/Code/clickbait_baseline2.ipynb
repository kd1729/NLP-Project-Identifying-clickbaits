{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "clickbait-baseline2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "tTHpCrayoRrL",
        "uk9N4filQelJ",
        "JmqJkVOqhxKw",
        "IIdzYNVWV08N",
        "OSwpUW5nV5X6",
        "QCKHVzheVXYG",
        "E1dZuCTRVbsD",
        "g0cTJg6DZ6Pu",
        "0INpbP4iZODT",
        "rovdxK3PmojF",
        "x-8lZ0S0-2aC",
        "Fp2x6ijU-u0L",
        "JHJSWJM1tN65",
        "O_JHFaVlmc5F"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HonvFAKKoYO-"
      },
      "source": [
        "# CARE-BERT: **C**lickb**a**it Detecto**r** using Self-attentiv**e** Network with **B**idirectional **E**ncoder **R**epresentations from **T**ransformers\n",
        "\n",
        "> ## EECS 498-004 Introduction to Natural Language Processing Course Project\n",
        "\n",
        "# Baseline 2: Headline Bi-LSTM with BERT embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTHpCrayoRrL"
      },
      "source": [
        "##  **0 - Setup**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "figwp7VpnIU5"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "dir = '/content/drive/Shareddrives/EECS 498-004 NLP Project - Clickbait/data/clickbait17/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szui5-R9nyaJ",
        "outputId": "4b43fb6e-3121-4a21-f285-e0b53ca5ab74"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/d5/f4157a376b8a79489a76ce6cfe147f4f3be1e029b7144fa7b8432e8acb26/transformers-4.4.2-py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 9.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 46.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 47.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=2532eb2f55e61fa6b575be9800a778500469984161e750d5923ac46a5aea372f\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iANs3wJRoFYF"
      },
      "source": [
        "# from transformers import pipeline; \n",
        "# print(pipeline('sentiment-analysis')('we love you'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uk9N4filQelJ"
      },
      "source": [
        "## **1 - Data Corpus**\n",
        "\n",
        "\n",
        "*   data = 19538\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zV2zvy5oQqf"
      },
      "source": [
        "class Webis17:\n",
        "    '''\n",
        "    self.corpus: (post, text, truthMean)\n",
        "    '''\n",
        "    def __init__(self, path):\n",
        "        self.train_file = path + 'instances.jsonl'\n",
        "        self.truth_file = path + 'truth.jsonl'\n",
        "        df_train = pd.read_json(self.train_file, lines=True)\n",
        "        df_truth = pd.read_json(self.truth_file, lines=True)\n",
        "        self.size = df_train.shape[0]\n",
        "\n",
        "        truth_id, truth_mean = list(df_truth['id']), list(df_truth['truthMean'])\n",
        "        truth_dict = {truth_id[i]:truth_mean[i] for i in range(self.size)}\n",
        "        train_id, train_post, train_text = list(df_train['id']), list(df_train['postText']), list(df_train['targetParagraphs'])\n",
        "        #? train_post[i] is a list\n",
        "        self.corpus = [(train_post[i][0], ' '.join(para for para in train_text[i]), truth_dict[train_id[i]]) for i in range(self.size)]\n",
        "\n",
        "        # print(self.corpus[:10])\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UMZFw6tQz2X",
        "outputId": "9d897c03-856f-4578-b0d9-2060d7d2afb5"
      },
      "source": [
        "# web17 = Webis17('./data/clickbait17/')\n",
        "dir = '/content/drive/Shareddrives/EECS 498-004 NLP Project - Clickbait/data/clickbait17/'\n",
        "web17 = Webis17(dir)\n",
        "num_data = len(web17.corpus)\n",
        "print(num_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19538\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpZSQAKFpswS",
        "outputId": "27f61454-5cde-4c44-ed2f-8c7cf96bbf07"
      },
      "source": [
        "print(web17.corpus[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('UK’s response to modern slavery leaving victims destitute while abusers go free', 'Thousands of modern slavery victims have\\xa0not come forward, while others who have chosen to report their enslavers have ended up destitute as a result of insufficient support, say\\xa0MPs “Inexcusable” failures in the UK’s system for dealing with modern slavery are\\xa0leaving victims reduced to destitution while their abusers go free because they are not adequately supported to testify against them, an alarming report has warned. Thousands of\\xa0victims\\xa0have not come forward, while others who have chosen to give evidence against their enslavers have ended up destitute as a result of insufficient support, according to\\xa0a report published\\xa0today by\\xa0the Work and Pensions Committee. It is estimated there are between 10,000 and 13,000 victims of modern slavery in the UK, but the report\\xa0warns that failings in the current mechanism mean\\xa0that once they are identified, they have no automatic formal immigration status or rights. Weak and uncoordinated frontline support means victims are often faced with a total lack of understanding or even recognition of their situation, which in turn has a negative impact on the number of successful prosecutions of slave masters, according to the findings. One, Client M, told the Committee he managed to escape from his traffickers, but it took four years before someone recognised that he was a potential victim of modern slavery. According to the report, his adviser in the Jobcentre, who knew his story, did not pick up on his situation, and it took four years before someone finally offered him the help he required. The report also found that no data is collected on victims once they leave the National Referral Mechanism (NRM) – the UK’s framework for identifying victims of human trafficking and ensuring they receive protection and support – and that the recording of data that is collected was “generally substandard”. The Committee said it was “unacceptable” that the Government did not monitor the re-trafficking of victims and urged that reform to the NRM must include the recording of instances where victims have been processed through the framework more than once. As part of the inquiry, Baroness Butler-Sloss, Trustee of the Human Trafficking Foundation who helped draft the modern slavery bill in 2014, said the outcome of the NRM\\xa0process was “nothing but a piece of paper” to victims. “It is an extremely unattractive anomaly and an extremely expensive process putting a person through the NRM to get a positive outcome that everybody accepts that person is the victim of an appalling crime,” she added. “At that stage, having spent all that money, having gone through all that process, there is no result except a piece of paper.” Problems surrounding asylum rights were also noted in the report, in its findings that\\xa0being\\xa0a victim of slavery through the NRM confers no equivalent right to remain for any period, while\\xa0recognition as a refugee grants an initial period of five years’ leave to remain in the UK. The damning findings come less than a year\\xa0after Theresa May announced\\xa0on entering Downing Street\\xa0that\\xa0Britain would\\xa0lead the fight against modern slavery,\\xa0calling\\xa0it “the greatest human rights issue of our time”, and pledging a £33m\\xa0boost to coordinating\\xa0the Government response to the issue. The inquiry, launched at the request of the UK’s Independent Anti-Slavery Commissioner after he wrote to the Committee expressing his concerns that the support for victims of modern slavery was “inadequate”, made a number of urgent\\xa0recommendations. It suggested that all confirmed victims of modern slavery should be given at least one year’s leave to remain with a personal plan for their recovery, which it said should act as a “social passport” to support for at least the 12-month period of leave to remain. Training on how to spot signs of slavery and deal sensitively with identified victims should\\xa0be greatly improved among frontline Department for Work and Pensions (DWP) staff, who are often not aware of modern slavery, the Committee said. The report also urged that the Government must undertake an urgent review of the benefit support available to victims, including those who are assisting the police with investigations. Kate Roberts, head of office at the Human Trafficking Foundation, welcomed the findings\\xa0and said changes were “much needed” in a system that currently leaves victims of modern slavery without support and unable to unpack the trauma they’ve been subjected to. “It’s\\xa0good to see such a detailed report that really shines a light on what happens to victims after\\xa0the identification of modern slavery. I really welcome the\\xa0recommendations – they are much needed,\"\\xa0Ms Roberts told The Independent. \"At the moment the \\'recovery period\\' is actually the period during which it is being\\xa0decided\\xa0whether they are victims or\\xa0not. I don\\'t see how it is a recovery period. If\\xa0there\\'s\\xa0so much uncertainty in your life and you\\'ve come from this extreme trauma, I don\\'t think you can really begin to unpack that trauma and recover when you\\'ve got all the uncertainty around the corner. “Until now, our whole system has just focused on the identification of victims. They go through this really quite traumatic process of having to disclose what’s happened to them and almost relive it, and then the decision is made that they are a victim and it doesn’t really make a difference. They think now they’\\'ll be okay because the Government believes them, but in fact\\xa0it often doesn’t mean anything. “If these recommendations\\xa0were implemented it really would change that. Many victims will take longer than a year to recover, but at least they would have time to get relevant advice\\xa0and put the necessary applications in.” In light of the findings,\\xa0chair of the Committee\\xa0Frank Field MP\\xa0said: “While we applaud the leading role the UK has taken in tackling this ‘barbaric crime’, as the Prime Minister has called it, when you consider what is at stake, there is a shocking lack of awareness and co-ordination in the frontline services dealing with modern slavery. “What these people go through is unimaginable, and yet it is happening, here and now, and our response seems almost lackadaisical: a paper exercise earning you recognition as having been enslaved, which then entitles you to almost nothing as far as we can see.\\xa0 “We don’t even record instances where the same person is thrown back into this hell, even though that is surely the clearest sign of the failures in our response. No society worth its salt can allow this to continue, or fail to support those who fall victim.” Mr Field urged the Prime Minister to go further in the Modern Slavery Act she brought in as Home Secretary in 2015, by conducting an urgent review and putting in place some “basic minimum safeguards” in order to support victims. “The Prime Minister now needs to open up a further front in her Modern Slavery Act,” he said. “The incoming Government must conduct an urgent review of our national response and put in place some basic minimum safeguards, status\\xa0that will allow a person to begin to rebuild a life, testify against their abuser if they feel able\\xa0and, above all, be protected from the unimaginable but real possibility of falling victim again.”', 0.13333333332)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmqJkVOqhxKw"
      },
      "source": [
        "## **2 - Dataset Preprocessing - BERT Embedding**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIdzYNVWV08N"
      },
      "source": [
        "### Download BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uL8OzMfGh9Sn"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "bert_tokenizer.save_pretrained(dir+'bert-base-uncased')\n",
        "bert_model.save_pretrained(dir+'bert-base-uncased')\n",
        "\n",
        "# it turns out that bert has limited token length of 512"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSwpUW5nV5X6"
      },
      "source": [
        "### Load BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jz1gDtVTB-fH"
      },
      "source": [
        "## load from files & tokenizer analysis\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(dir+'bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained(dir+'bert-base-uncased')\n",
        "\n",
        "# encode1 = torch.tensor(bert_tokenizer.encode(web17.corpus[0][0]))\n",
        "# encode2 = torch.tensor(bert_tokenizer.encode(web17.corpus[1][0]))\n",
        "# encode3 = torch.tensor(bert_tokenizer.encode(web17.corpus[2][0]))\n",
        "# encodeAll = bert_tokenizer([web17.corpus[0][0], web17.corpus[1][0],web17.corpus[2][0]], padding=True,return_token_type_ids=False, return_attention_mask=False)['input_ids']\n",
        "\n",
        "# encodeAll_crop = bert_tokenizer([web17.corpus[0][0], web17.corpus[1][0],web17.corpus[2][0]], padding=True, truncation=True,max_length=20,return_token_type_ids=False, return_attention_mask=False)['input_ids']\n",
        "\n",
        "\n",
        "# print(encode1.shape)\n",
        "# print(encode2.shape)\n",
        "# print(encode3.shape)\n",
        "# print([len(lst)  for lst in encodeAll])\n",
        "# print([len(lst)  for lst in encodeAll_crop])\n",
        "\n",
        "\n",
        "# print(encode1)\n",
        "# print(encodeAll[0])\n",
        "# print(encodeAll_crop[0])\n",
        "\n",
        "# print(encode3)\n",
        "# print(encodeAll[2])\n",
        "# print(encodeAll_crop[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCKHVzheVXYG"
      },
      "source": [
        "### data profiling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhfONhzF9Yv-"
      },
      "source": [
        "## extract data\n",
        "title_all = [data[0] for data in web17.corpus]\n",
        "content_all = [data[1] for data in web17.corpus]\n",
        "score_all = torch.tensor([data[2] for data in web17.corpus], requires_grad=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAviW2QVTiSI",
        "outputId": "97355c96-3367-46a7-cb89-7be3093a282a"
      },
      "source": [
        "# title profiling\n",
        "\n",
        "title_all_tokenized_raw = bert_tokenizer(title_all,return_token_type_ids=False, return_attention_mask=False)['input_ids']\n",
        "print(max([len(lst) for lst in title_all_tokenized_raw ]))\n",
        "print(f\"Average # of tokens = {np.mean([len(lst) for lst in title_all_tokenized_raw])}\")\n",
        "print(f\"max # of tokens = {max([len(lst) for lst in title_all_tokenized_raw])}\")\n",
        "print(f\"ID of title with max # of tokens = {np.argmax([len(lst) for lst in title_all_tokenized_raw ])}\")\n",
        "print(\"---the title---\")\n",
        "print(title_all[np.argmax([len(lst) for lst in title_all_tokenized_raw ])])\n",
        "print(\"---the title---\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "104\n",
            "Average # of tokens = 17.628058143105743\n",
            "max # of tokens = 104\n",
            "ID of title with max # of tokens = 16508\n",
            "---the title---\n",
            "................\n",
            "................\n",
            "................\n",
            "................\n",
            "................\n",
            "................\n",
            "Okay, then...\n",
            "---the title---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wioz49Czom0D",
        "outputId": "066216c0-e4a3-44f1-8ad2-d9a55eb41a44"
      },
      "source": [
        "# content profiling\n",
        "content_all_tokenized_raw = bert_tokenizer(content_all,return_token_type_ids=False, return_attention_mask=False)['input_ids']\n",
        "print(f\"Average # of tokens = {np.mean([len(lst) for lst in content_all_tokenized_raw])}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average # of tokens = 791.2599037772546\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnb4iJ8nryAE",
        "outputId": "e92aeb4d-c0b0-4965-cfef-eeb37f2618b4"
      },
      "source": [
        "print(f\"max # of tokens = {max([len(lst) for lst in content_all_tokenized_raw])}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max # of tokens = 43357\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1dZuCTRVbsD"
      },
      "source": [
        "### extract embeddings & divide train/val/test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0cTJg6DZ6Pu"
      },
      "source": [
        "#### Raw"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77uRP31rRFE_",
        "outputId": "3d68d80e-ea14-44c1-fb77-debbd8c84046"
      },
      "source": [
        "# All embeddings\n",
        "title_all_tokenized = bert_tokenizer(title_all, padding=True,truncation=True,max_length=20, return_token_type_ids=False, return_attention_mask=False, return_tensors=\"pt\")['input_ids']\n",
        "print(title_all_tokenized.shape)\n",
        "print(title_all_tokenized)\n",
        "torch.save(title_all_tokenized, dir+'titles_tokens.pt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([19538, 20])\n",
            "tensor([[  101,  2866,  1521,  ...,  2489,   102,     0],\n",
            "        [  101,  2023,  2003,  ...,     0,     0,     0],\n",
            "        [  101,  1996,  1000,  ...,  1996,  2047,   102],\n",
            "        ...,\n",
            "        [  101,  2413,  2015,  ...,  2112,  1997,   102],\n",
            "        [  101,  2821,  5076,  ...,     0,     0,     0],\n",
            "        [  101,  2957, 11011,  ...,     0,     0,     0]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gn1Vehxz55j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07ff3412-73c2-4d47-c829-4a6d5553c596"
      },
      "source": [
        "train_size = 700\n",
        "val_size = 100\n",
        "outputs = bert_model(title_all_tokenized[:(train_size+val_size), :])\n",
        "title_all_embed = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
        "print(title_all_embed.shape) # batchsize x # tokens of sent x embed_dim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([800, 20, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJ2RloihoNfv"
      },
      "source": [
        "## tokenize paragraphs -> longformer\n",
        "\n",
        "# from transformers import LongformerModel, LongformerTokenizer\n",
        "# long_tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
        "# long_model = LongformerModel.from_pretrained('allenai/longformer-base-4096')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJtRHbxtvD9V"
      },
      "source": [
        "# content_all_tokenized = bert_tokenizer(content_all, padding=True,return_token_type_ids=False, return_attention_mask=False, return_tensors=\"pt\")['input_ids']\n",
        "# print(content_all_tokenized.shape)\n",
        "\n",
        "# outputs = bert_model(content_all_tokenized)\n",
        "# content_all_embed = outputs[0]\n",
        "# torch.save(content_all_embed, dir+'/contents.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0INpbP4iZODT"
      },
      "source": [
        "#### Process by patches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBt7BeV-ZLeF",
        "outputId": "63fded81-d290-42db-d82b-6ae2de561e3f"
      },
      "source": [
        "import torch\n",
        "title_all_tokenized = torch.load(dir+'titles_tokens.pt')\n",
        "print(title_all_tokenized.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([19538, 20])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y24e_8W6aZlA",
        "outputId": "76f15bda-4a5f-4ced-9707-919270b3e618"
      },
      "source": [
        "import gc\n",
        "\n",
        "num_data = 19538\n",
        "extract_size = 800\n",
        "for i in range(num_data//800):\n",
        "    outputs = bert_model(title_all_tokenized[(extract_size*i):(extract_size*(i+1)), :])\n",
        "    title_all_embed = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
        "    print(title_all_embed.shape) # batchsize x # tokens of sent x embed_dim\n",
        "    print(f\"From size {str(extract_size*i)} to {str(extract_size*(i+1))}\")\n",
        "    # save Data\n",
        "    torch.save(title_all_embed, dir+'/titles_'+str(extract_size*i)+'_'+str(extract_size*(i+1)))\n",
        "    del outputs\n",
        "    del title_all_embed\n",
        "    gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([800, 20, 768])\n",
            "From size 0 to 800\n",
            "torch.Size([800, 20, 768])\n",
            "From size 800 to 1600\n",
            "torch.Size([800, 20, 768])\n",
            "From size 1600 to 2400\n",
            "torch.Size([800, 20, 768])\n",
            "From size 2400 to 3200\n",
            "torch.Size([800, 20, 768])\n",
            "From size 3200 to 4000\n",
            "torch.Size([800, 20, 768])\n",
            "From size 4000 to 4800\n",
            "torch.Size([800, 20, 768])\n",
            "From size 4800 to 5600\n",
            "torch.Size([800, 20, 768])\n",
            "From size 5600 to 6400\n",
            "torch.Size([800, 20, 768])\n",
            "From size 6400 to 7200\n",
            "torch.Size([800, 20, 768])\n",
            "From size 7200 to 8000\n",
            "torch.Size([800, 20, 768])\n",
            "From size 8000 to 8800\n",
            "torch.Size([800, 20, 768])\n",
            "From size 8800 to 9600\n",
            "torch.Size([800, 20, 768])\n",
            "From size 9600 to 10400\n",
            "torch.Size([800, 20, 768])\n",
            "From size 10400 to 11200\n",
            "torch.Size([800, 20, 768])\n",
            "From size 11200 to 12000\n",
            "torch.Size([800, 20, 768])\n",
            "From size 12000 to 12800\n",
            "torch.Size([800, 20, 768])\n",
            "From size 12800 to 13600\n",
            "torch.Size([800, 20, 768])\n",
            "From size 13600 to 14400\n",
            "torch.Size([800, 20, 768])\n",
            "From size 14400 to 15200\n",
            "torch.Size([800, 20, 768])\n",
            "From size 15200 to 16000\n",
            "torch.Size([800, 20, 768])\n",
            "From size 16000 to 16800\n",
            "torch.Size([800, 20, 768])\n",
            "From size 16800 to 17600\n",
            "torch.Size([800, 20, 768])\n",
            "From size 17600 to 18400\n",
            "torch.Size([800, 20, 768])\n",
            "From size 18400 to 19200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mI7gE1W1deXU",
        "outputId": "be9f2b78-57cf-464d-e9b6-b34ba4c0fa49"
      },
      "source": [
        "# last portion\n",
        "\n",
        "num_patchs = num_data//extract_size\n",
        "outputs = bert_model(title_all_tokenized[(extract_size*num_patchs):, :])\n",
        "title_all_embed = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
        "print(title_all_embed.shape) # batchsize x # tokens of sent x embed_dim\n",
        "print(f\"From size {str(extract_size*num_patchs)} to {str(num_data)}\")\n",
        "# save Data\n",
        "torch.save(title_all_embed, dir+'/titles_'+str(extract_size*num_patchs)+'_'+str(num_data))\n",
        "\n",
        "del outputs\n",
        "del title_all_embed\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([338, 20, 768])\n",
            "From size 19200 to 19538\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "501"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qPZ-rxof0mO"
      },
      "source": [
        "#### Combine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJgMz1-2f3tV",
        "outputId": "24518df8-4be6-4120-803f-b10b866f3524"
      },
      "source": [
        "Xt = torch.zeros(num_data, 20, 768)\n",
        "for i in range(num_data//800):\n",
        "    # curr_Xt = torch.load(dir+'/titles_'+str(extract_size*i)+'_'+str(extract_size*(i+1)))\n",
        "    Xt[extract_size*i:extract_size*(i+1), :,: ] = torch.load(dir+'/titles_'+str(extract_size*i)+'_'+str(extract_size*(i+1)))\n",
        "Xt[extract_size*num_patchs:,:,:] = torch.load(dir+'/titles_'+str(extract_size*num_patchs)+'_'+str(num_data))\n",
        "\n",
        "print(Xt.shape)\n",
        "# print(Xt[-10:,:,:])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([19538, 20, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJuyUOcqhJ24"
      },
      "source": [
        "torch.save(Xt, dir+'/titles_all.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rovdxK3PmojF"
      },
      "source": [
        "##**3 - Load Data: Ready for Training!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-8lZ0S0-2aC"
      },
      "source": [
        "### All (20 tokens)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24-hCxNN_Gn-",
        "outputId": "124458be-d934-4524-da45-7042d98a07f6"
      },
      "source": [
        "# load data\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "dir = '/content/drive/Shareddrives/EECS 498-004 NLP Project - Clickbait/data/clickbait17/'\n",
        "Xt_all = torch.load(dir+'/titles_all.pt')\n",
        "yt_all = torch.load(dir+'/scores.pt')\n",
        "print(Xt_all.shape)\n",
        "print(yt_all.shape)\n",
        "\n",
        "num_data = Xt_all.shape[0]\n",
        "train_size = 16000\n",
        "val_size = 2000\n",
        "test_size = num_data - train_size - val_size\n",
        "batch_size = 64\n",
        "train_set = TensorDataset(Xt_all[:train_size,:,:], yt_all[:train_size])\n",
        "val_set = TensorDataset(Xt_all[train_size:train_size+val_size,:,:], yt_all[train_size:train_size+val_size])\n",
        "test_set = TensorDataset(Xt_all[train_size+val_size:,:,:], yt_all[train_size+val_size:])\n",
        "\n",
        "train_dataloader = DataLoader(train_set, batch_size=batch_size)\n",
        "val_dataloader = DataLoader(val_set, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_set, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([19538, 20, 768])\n",
            "torch.Size([19538])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fp2x6ijU-u0L"
      },
      "source": [
        "### Only [CLS]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6B0YhNTJ-ojt",
        "outputId": "3a89e552-1693-41c4-f894-f6cd290160f4"
      },
      "source": [
        "# load data\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "dir = '/content/drive/Shareddrives/EECS 498-004 NLP Project - Clickbait/data/clickbait17/'\n",
        "Xt_all = torch.load(dir+'/titles_all.pt')\n",
        "yt_all = torch.load(dir+'/scores.pt')\n",
        "print(Xt_all.shape)\n",
        "print(yt_all.shape)\n",
        "\n",
        "num_data = Xt_all.shape[0]\n",
        "train_size = 16000\n",
        "val_size = 2000\n",
        "test_size = num_data - train_size - val_size\n",
        "batch_size = 64\n",
        "train_set = TensorDataset(Xt_all[:train_size,0,:], yt_all[:train_size])\n",
        "val_set = TensorDataset(Xt_all[train_size:train_size+val_size,0,:], yt_all[train_size:train_size+val_size])\n",
        "test_set = TensorDataset(Xt_all[train_size+val_size:,0,:], yt_all[train_size+val_size:])\n",
        "\n",
        "train_dataloader = DataLoader(train_set, batch_size=batch_size)\n",
        "val_dataloader = DataLoader(val_set, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_set, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([19538, 20, 768])\n",
            "torch.Size([19538])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aEzlZBGr87h"
      },
      "source": [
        "## **4 - Model 1 - Simple LSTM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHJSWJM1tN65"
      },
      "source": [
        "### Model Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9s5Y0cJD06wq"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, batch_size, num_tokens, embed_dim, hidden_dim,  n_layers = 1, dropout = 0.0):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.lstm=nn.LSTM(embed_dim, hidden_dim, n_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
        "        self.flatten = nn.Flatten(1)\n",
        "        # self.fc1=nn.Linear(num_tokens*hidden_dim, 64)\n",
        "        # self.fc1=nn.Linear(num_tokens*hidden_dim, 1)\n",
        "        # take CLS token, birection\n",
        "        self.fc1=nn.Linear(2*hidden_dim, 64)\n",
        "\n",
        "        self.fc2=nn.Linear(64, 1)\n",
        "        \n",
        "    def forward(self, x, hidden):\n",
        "        '''\n",
        "            x: batch_size x num_tokens x embed_dim\n",
        "        '''\n",
        "        # take CLS token\n",
        "        # print(x[:,0,:].unsqueeze(1).shape)\n",
        "        lstm_out, hidden = self.lstm(x.unsqueeze(1), hidden) # batch_size x 1 x (2*hidden_dim)\n",
        "\n",
        "        # flat = self.flatten(lstm_out) \n",
        "        flat = lstm_out.squeeze() # batch_size x hidden_dim\n",
        "\n",
        "        out1 = self.fc1(flat) # batch_size x 64\n",
        "        out2 = self.fc2(torch.relu(out1)) # batch_size x 1\n",
        "        out = torch.sigmoid(out2)\n",
        "\n",
        "        # # single layer\n",
        "        # out = torch.sigmoid(out1)\n",
        "        return out, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        # birections -> *2\n",
        "        hidden = (weight.new(self.n_layers*2, batch_size, self.hidden_dim).zero_().to(device),\n",
        "                      weight.new(self.n_layers*2, batch_size, self.hidden_dim).zero_().to(device))\n",
        "        return hidden\n",
        "\n",
        "def init_weights(m):\n",
        "    '''\n",
        "    Initialize weights\n",
        "    '''\n",
        "    if isinstance(m, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.fill_(0.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iy68yxN8CKf",
        "outputId": "824dfd53-1b89-4013-9739-7a8dd2d73874"
      },
      "source": [
        "# load GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))\n",
        "print(torch.cuda.get_device_name(0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda device\n",
            "Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_JHFaVlmc5F"
      },
      "source": [
        "### Hyperparamters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iLV00TB6siQ",
        "outputId": "319b656c-e395-45e1-ddd0-7f411064e5ba"
      },
      "source": [
        "\n",
        "\n",
        "hidden_dim = 10 # num of tokens is typically 20\n",
        "_ , num_tokens, embed_dim = Xt_all.shape\n",
        "# dropout = 0.0\n",
        "dropout = 0.2\n",
        "\n",
        "model = LSTM(batch_size, num_tokens, embed_dim, hidden_dim, n_layers=2, dropout = dropout).to(device)\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau # learning rate scheduler\n",
        "lr_scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.25, patience=0, threshold=0.05,min_lr=3e-5, verbose=True)\n",
        "\n",
        "model.apply(init_weights)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTM(\n",
              "  (lstm): LSTM(768, 10, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (fc1): Linear(in_features=20, out_features=64, bias=True)\n",
              "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LdtXl0UtcT8"
      },
      "source": [
        "### Training & Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGx0DkdJnN38"
      },
      "source": [
        "#### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdb2uaSH-7RH"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "### Training ###\n",
        "def train(train_dataloader, y_truth, model, loss_fn, optimizer, mute = False):\n",
        "    model.train()\n",
        "\n",
        "    size = len(train_dataloader.dataset)\n",
        "\n",
        "    y_pred_train = []\n",
        "    for batch, (X, y) in enumerate(train_dataloader):\n",
        "        hidden = model.init_hidden(X.shape[0])\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred, hidden = model(X, hidden)\n",
        "        y_pred_train.extend(pred.squeeze().cpu())\n",
        "        loss = loss_fn(pred.squeeze(), y)\n",
        "        # Backpropagation\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 20 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            if not mute:\n",
        "                print(f\"Training loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "    y_pred_train = torch.tensor(y_pred_train, dtype=float)\n",
        "    performance = loss_fn(y_pred_train, y_truth)\n",
        "    clf_performance = ((y_pred_train>0.5)==(y_truth>0.5)).float().mean()\n",
        "\n",
        "    if not mute:\n",
        "        print(f\"Training Loss: {performance}\")\n",
        "        print(f\"Training Classifier Accuracy: {clf_performance}\")\n",
        "    return y_pred_train\n",
        "\n",
        "### Testing ###\n",
        "def test(val_dataloader, y_truth, model, loss_fn, lr_scheduler, mute = False, mode = 0):\n",
        "    '''\n",
        "    mode = 0: validation when training (lr_scheduler)\n",
        "    mode = 1: validation\n",
        "    mode = 2: test\n",
        "    '''\n",
        "    hidden_val = model.init_hidden(batch_size)\n",
        "    model.eval()\n",
        "\n",
        "    y_pred_val = []\n",
        "    for batch, (X, y) in enumerate(val_dataloader):\n",
        "        hidden_val = model.init_hidden(X.shape[0])\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        pred, hidden_val = model(X, hidden_val)\n",
        "        y_pred_val.extend(pred.squeeze().cpu())\n",
        "\n",
        "    y_pred_val = torch.tensor(y_pred_val, dtype=float)\n",
        "    performance = loss_fn(y_pred_val, y_truth)\n",
        "    if mode == 0:\n",
        "        lr_scheduler.step(performance)\n",
        "    clf_performance = ((y_pred_val>0.5)==(y_truth>0.5)).float().mean()\n",
        "\n",
        "    f1_performance = f1_score((y_pred_val>0.5).float().numpy(), (y_truth>0.5).float().numpy())\n",
        "    p_performance = pearsonr(y_pred_val.detach().numpy(), y_truth.detach().numpy())[0]\n",
        "    if not mute:\n",
        "        if mode == 2:\n",
        "            print(f\"Test Loss: {performance}\")\n",
        "            print(f\"Test Accuracy: {clf_performance}\")\n",
        "            print(f\"Test F1 Score: {f1_performance}\")\n",
        "            print(f\"Test Pearson Coefficient: {p_performance}\")\n",
        "        else:\n",
        "            print(f\"Validation Loss: {performance}\")\n",
        "            print(f\"Validation Accuracy: {clf_performance}\")\n",
        "            print(f\"Validation F1 Score: {f1_performance}\")\n",
        "            print(f\"Test Pearson Coefficient: {p_performance}\")\n",
        "\n",
        "    return performance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MqER9wCnROB"
      },
      "source": [
        "#### On-Going"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4yKcGmp-OBA",
        "outputId": "2401c157-5141-4d7e-89a6-93843e4ec666"
      },
      "source": [
        "## Training & validation\n",
        "\n",
        "### ESTIMATED TIME: 2\n",
        "# num * 20 * 768 -> 1 min per batch -> 2 hr per epoch\n",
        "# CLS -> num * 1 * 768, hidden = 10, bidirectional -> 8 min per epoch\n",
        "###\n",
        "\n",
        "\n",
        "epochs = 50\n",
        "model.train()\n",
        "\n",
        "best_val_performance = 1.0 # any number works\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, yt_all[:train_size], model, loss_fn, optimizer)\n",
        "    val_performance = test(val_dataloader, yt_all[train_size:train_size+val_size], model, loss_fn, lr_scheduler)\n",
        "\n",
        "    if val_performance < best_val_performance:\n",
        "        best_val_performance = val_performance\n",
        "        print(f'NEW BEST MODEL! Performance: {best_val_performance}')\n",
        "        torch.save(model, dir+'/best_model')\n",
        "print(\"Done!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Training loss: 0.032313  [    0/16000]\n",
            "Training loss: 0.021919  [ 1280/16000]\n",
            "Training loss: 0.030513  [ 2560/16000]\n",
            "Training loss: 0.032846  [ 3840/16000]\n",
            "Training loss: 0.026816  [ 5120/16000]\n",
            "Training loss: 0.022634  [ 6400/16000]\n",
            "Training loss: 0.018584  [ 7680/16000]\n",
            "Training loss: 0.033550  [ 8960/16000]\n",
            "Training loss: 0.020973  [10240/16000]\n",
            "Training loss: 0.025444  [11520/16000]\n",
            "Training loss: 0.028835  [12800/16000]\n",
            "Training loss: 0.031509  [14080/16000]\n",
            "Training loss: 0.023814  [15360/16000]\n",
            "Training Loss: 0.029822561029702536\n",
            "Training Classifier Accuracy: 0.8504999876022339\n",
            "Epoch     2: reducing learning rate of group 0 to 7.5000e-05.\n",
            "Validation Loss: 0.032548400412765674\n",
            "Validation Accuracy: 0.847000002861023\n",
            "NEW BEST MODEL! Performance: 0.032548400412765674\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Training loss: 0.029771  [    0/16000]\n",
            "Training loss: 0.022390  [ 1280/16000]\n",
            "Training loss: 0.027395  [ 2560/16000]\n",
            "Training loss: 0.030946  [ 3840/16000]\n",
            "Training loss: 0.028228  [ 5120/16000]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDAgVSHbkafR"
      },
      "source": [
        "###\n",
        "### Naming Rules: <>\n",
        "###\n",
        "\n",
        "# torch.save(model, dir+'model_20')\n",
        "\n",
        "# CLS, num * 1 * 768, hidden = 10, bidirectional -> 8 min per epoch\n",
        "torch.save(model, dir+'model_CLS_10_bi')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTk3CbALcp6L"
      },
      "source": [
        "import torch\n",
        "\n",
        "dir = '/content/drive/Shareddrives/EECS 498-004 NLP Project - Clickbait/data/clickbait17/'\n",
        "\n",
        "hidden_dim = 10 # num of tokens is typically 20\n",
        "_ , num_tokens, embed_dim = Xt_all.shape\n",
        "# dropout = 0.0\n",
        "dropout = 0.2\n",
        "\n",
        "model = LSTM(batch_size, num_tokens, embed_dim, hidden_dim, n_layers=2, dropout = dropout).to(device)\n",
        "model = torch.load(dir+'/best_model')\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau # learning rate scheduler\n",
        "lr_scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.25, patience=0, threshold=0.05,min_lr=3e-5, verbose=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9jXyzgoksmy",
        "outputId": "c04a34b0-fff4-4fb5-95f8-795a9f309e71"
      },
      "source": [
        "_ = test(val_dataloader, yt_all[train_size:train_size+val_size], model, loss_fn, lr_scheduler, mode = 1)\n",
        "_ = test(test_dataloader, yt_all[train_size+val_size:], model, loss_fn, lr_scheduler, mode = 2)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation Loss: 0.03254840028757948\n",
            "Validation Accuracy: 0.847000002861023\n",
            "Validation F1 Score: 0.6592427616926503\n",
            "Test Pearson Coefficient: 0.7251489583277191\n",
            "Test Loss: 0.03121176758478534\n",
            "Test Accuracy: 0.8582574725151062\n",
            "Test F1 Score: 0.6736526946107785\n",
            "Test Pearson Coefficient: 0.731791129078822\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}